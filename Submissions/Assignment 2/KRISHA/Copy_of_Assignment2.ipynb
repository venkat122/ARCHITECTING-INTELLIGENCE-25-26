{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "I5gUKs5M9tIN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "# -----------------------------\n",
        "# Positional Encoding Layer\n",
        "# -----------------------------\n",
        "\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super().__init__()\n",
        "        self.pos_encoding = self.positional_encoding(max_len, d_model)\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angle_rates = 1 / tf.pow(10000.0, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return pos * angle_rates\n",
        "\n",
        "    def positional_encoding(self, max_len, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            pos=tf.range(max_len)[:, tf.newaxis],\n",
        "            i=tf.range(d_model)[tf.newaxis, :],\n",
        "            d_model=d_model\n",
        "        )\n",
        "\n",
        "        # apply sin to even indices, cos to odd indices\n",
        "        angle_rads = tf.where(\n",
        "            tf.range(d_model)[tf.newaxis, :] % 2 == 0,\n",
        "            tf.sin(angle_rads),\n",
        "            tf.cos(angle_rads)\n",
        "        )\n",
        "\n",
        "        return angle_rads[tf.newaxis, ...]\n",
        "\n",
        "    def call(self, x):\n",
        "        return x + tf.cast(self.pos_encoding[:, :tf.shape(x)[1], :], x.dtype)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# -----------------------------\n",
        "# Transformer Encoder Block\n",
        "# -----------------------------\n",
        "def transformer_encoder_block(embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "    inputs = layers.Input(shape=(None, embed_dim))\n",
        "\n",
        "    # Layer Norm + Multi-Head Self Attention\n",
        "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=embed_dim // num_heads,\n",
        "        dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Add()([inputs, attention_output])  # Residual connection\n",
        "\n",
        "    # Feed Forward Network\n",
        "    ff = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "    ff = layers.Dense(ff_dim, activation=\"relu\")(ff)\n",
        "    ff = layers.Dense(embed_dim)(ff)\n",
        "    ff = layers.Dropout(dropout)(ff)\n",
        "    x = layers.Add()([x, ff])  # Residual connection\n",
        "\n",
        "    return Model(inputs, x, name=\"TransformerEncoderBlock\")\n"
      ],
      "metadata": {
        "id": "XXFBhT1r931I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# -----------------------------\n",
        "# Build the Transformer Model\n",
        "# -----------------------------\n",
        "def build_transformer(\n",
        "        max_len=100,\n",
        "        vocab_size=10000,\n",
        "        embed_dim=64,\n",
        "        num_heads=4,\n",
        "        ff_dim=128,\n",
        "        num_layers=3\n",
        "    ):\n",
        "    inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "\n",
        "    # Token Embedding\n",
        "    x = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)\n",
        "\n",
        "    # Positional Encoding\n",
        "    x = PositionalEncoding(max_len, embed_dim)(x)\n",
        "\n",
        "    # Transformer Encoder Blocks\n",
        "    for _ in range(num_layers):\n",
        "        x = transformer_encoder_block(embed_dim, num_heads, ff_dim)(x)\n",
        "\n",
        "    # Classification head\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs, outputs, name=\"Transformer_3Layer\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "2xdidbIx_Ajb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_angles(self, pos, i, d_model):\n",
        "    i = tf.cast(i, tf.float32)\n",
        "    angle_rates = 1 / tf.pow(\n",
        "        10000.0, (2 * (i // 2)) / tf.cast(d_model, tf.float32)\n",
        "    )\n",
        "    return pos * angle_rates\n"
      ],
      "metadata": {
        "id": "IFLZEIXh_qul"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}